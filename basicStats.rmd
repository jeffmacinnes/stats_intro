---
title: "Basic Stats"
date: "January 27, 2017"
header-includes:
   - \usepackage{bbm}
output:
  github_document:
    toc: true
---

The goal here is to provide an overview on some basic statistical approaches that you can use in analyzing your AVB eye-tracking data. We'll cover the ideas behind these approaches -- identifying dependent/independent variables, deciding on appropriate analyses, interpretting results -- as well as the tools to run these analyses yourself. We'll be using [RStudio](https://www.rstudio.com/products/rstudio/download/), so if you haven't already download and install both [R](https://cran.r-project.org/) and [RStudio](https://www.rstudio.com/products/rstudio/download/).

# The Data
In order to have some data to play with, there is a simulated datafile in `data/exampleData.tsv`

This dataset has 40 subjects. For each subject we have values for IQ, and fixation time on the nose of various image categories. So we have:

* Dependent Variables (DV):
    + __IQ__: subject IQ score
    + __noseDwellTime__: time spent fixating on the nose
  
And we measured __noseDwellTime__ across 3 different image categories. So, one independent variable (IV) with 3 levels

* Independent Variable levels:
    + __movie__: faces of movie stars
    + __athlete__: faces of athletes
    + __nobel__: faces of Nobel laureates


So, let's load the dataset:
```{r}
# if you haven't already, set your working directory to 'stats_intro'
#setwd("<dirPath>/stats_intro")

# read the data, store in variable called 'dt'
dt <- read.csv('data/exampleData.tsv', sep='\t', row.names=1)

# print the first few rows
head(dt, n=5)
```

You can see all of our variables. Each subject has an IQ score and average noseDwellTimes for Movie stars, Athletes, and Nobel. 

Notice that our table has one row per subject, and multiple *observations* for each subject in different columns. When the table is organized like this, it is referred to as *wide-format* 

To access specific columns of your data, use the following format:
```{r}
# <tableName>$<columnName>
dt$IQ
```

# Descriptive Statistics
Descriptive statistics are ways to summarize *your data*. That is, they don't infer any conclusions from your data to the larger population from which your subjects are drawn from (you'll need **inferential statistics** for that...coming later). Descriptive statistics can be things like the *mean*, *mode*, and *median* of a variable. 

### Quick data summary
Let's get some basic descriptive stats on this dataset.

```{r}
# if you need to install the psych library:
# install.packages("psych", dependencies=TRUE)
library(psych)  # this is an external library that has some really useful tools within it

# the 'describe' function comes with the psych library and will report descriptive stats on all vars in your table
describe(dt)
```

You can see, for instance, that subjects spent an average of __249.81__ ms looking at the noses of movie stars, and __342.80__ ms looking at the noses of Nobel laureates. In this sample, participants spent longer (on average) looking at the noses of Nobel winners than movie stars. But that's all you can say. You cannot generalize that conclusion to the population at large. 

### Plotting raw data
It's often useful to plot your raw data to give you a better look at it. For one-dimensional data (e.g. subject IQ scores), a histogram can be a useful way to visualize your data
```{r warning=FALSE, message=FALSE} 
# if you need to install the ggplot library:
# install.packages("ggplot2", dependencies=TRUE)
library(ggplot2)  # useful plotting library

qplot(IQ, data=dt, 
      binwidth=5,
      main="Histogram for IQ (40 samples)",
      col=I("white"))
```

This shows the number of particpants (y-axis) whose IQ was within a certain IQ range (x-axis). Try plotting the distributions of other variables. By looking at the histogram, you can get a quick sense of whether a given variable is __normally distributed__. Many of the statistical tests described below rely and assume on data points being normally distributed. So what's that mean?

### Normal Distributions
If a variable is __normally distributed__, its histogram will take on a bell-shaped curve. Most of the data points will be clustered around the mean, i.e. the __count__ (i.e. y-axis) will have the greatest value near the mean. As you move outward toward the tails in either direction, the counts will drop off symmetrically. This means you have ever fewer datapoints at extreme values along the x-axis. 

Let's look at our IQ scores again. Above, we plotted the histogram of IQ scores in our sample of 40 subjects. Let's plot the histogram of IQ scores from a larger population, say 500 subjects.

```{r}
# read in larger population datatable
dt_pop <- read.csv('data/exampleData_population.tsv', sep='\t', row.names=1)


# plot the histogram for IQ scores from larger population
ggplot(dt_pop, aes(x=IQ, y=..density..)) + 
  geom_histogram(binwidth=5, size=.2, col="white") + 
  geom_density(colour="black") +
  geom_vline(xintercept=mean(dt_pop$IQ), color="red") +
  ggtitle("Histogram for IQ (500 samples)")

```

With 500 datapoints it becomes easier to see that IQ scores appear normally distributed. IQ scores are clustered symmetrically about the mean (shown as vertical red line). Offically, for a distribution to be considered __normal__:
  
  + __68%__ of datapoints must fall within +/- 1 standard deviation of the mean
  + __95.5%__ of datapoints must fall within +/- 2 standard deviations of the mean
  + the remaining __4.5%__ of datapoints are in the tails (__~2.3%__ in each tail)

Simply saying a distribution *looks* normal isn't enough. Ideally, we'd like to be able to *quantify* our evidence for making that claim. Testing whether a distribution is normal or not is a good segue into talking about __inferential statistics__. 

# Inferential Statistics
Inferential statistics are used to *infer* aspects about a population based on the *sample* of data you measured. You want to know if people are taller in Durham vs. Chapel Hill? You collect a random sample of 200 individuals from both locations, measure their heights, and come up with an average height of individuals from Durham and an average height of individuals from Chapel Hill. Say you measure that Durhamites are 3cm taller. Is this a __true__ difference, or just a result of the individuals you happened to sample? If you sampled 200 *different* people, how likely would you be to get the same response? This is where __inferential statistics__ come into play.

__So... are the IQ scores from our sample normally distributed or not?__

Here's the plot again

```{r echo=FALSE}
qplot(IQ, data=dt, 
      binwidth=5,
      main="Histogram for IQ (40 samples)",
      col=I("white"))
```

The Shapiro-Wilk normality test is a way of testing how liklely it is your observed data came from a normal distribution
```{r}
shapiro.test(dt$IQ)

```

This test, like many of the statistical tests covered below, returns a __p-value__. Knowing how to interpret a p-value is critical to understanding your results, and crucially, the limitations of your conclusions. To interpret a p-value, you need to be familiar with the idea of hypothesis testing.

### Hypothesis testing
With inferential statistics, the __true__ state of the world is unknown. You don't know the __true__ average height of everyone in Durham (unless you were to measure everyone, in which case you don't need stats). What you are left with is __probabilities__. How likely is it that my observations *reflect* the true state of the world? 

If we're thinking about our normal distribution question above, we have two possibilities. Or let's call them *hypotheses*:

  + __hypothesis 1:__ the data __really are__ normally distributed
  + __hypothesis 2:__ the data __really are not__ normally distributed
  
You can either conclude that hypothesis 1 is correct, or conclude that hypothesis 2 is correct. So, there are 4 possible outcomes:

|               | __truth:__ normal   | __truth:__ not normal  |
| ------------- |:-------------:| -----:|
| __conclude: normal__      | correct | incorrect (false positive) |
| __conclude: not normal__       | incorrect (false negative)      |   correct |

To set up a hypothesis test, we define one of our hypothesis as the __null__ hypothesis (called __H~o~__) and the other the __alternative__ hypothesis (__H~A~__). In the case of the Shapiro-Wilk normality test, we have:

  + __H~o~__: Our IQ scores came from a normally distributed population
  + __H~A~__: Our IQ scores did not come from a normally distributed population
  
So our null hypothesis is that our data indeed came from a normally distributed population. Remember, we don't have any way of knowing if this is __actually__ true or not. Instead, we say "assuming the data __DID__ come from a normally distributed population, how probable is it that we would have gotten our observed data through random sampling?" This probability is reflected in the __p-value__. 

When we run a hypothesis test we are asking whether we are going to __reject__ or __fail to reject__ the null hypothesis based on the strength of the p-value. So we also need to set up a p-value threshold that will allow us to make that decision. 

Our threshold (also sometimes called __*alpha*__, or $\alpha$) can be whatever we set it as. However, by convention you'll often see the following thresolds used to define a result as "significant" or "not":

  + __*p*__ $\leq$ __0.05__
  + __*p*__ $\leq$ __0.01__
  + __*p*__ $\leq$ __0.001__

When we run a statistical test and get a __p-value__ we can compare it against our __alpha__ and determine whether our result reached "significance" or not. For example, if our __alpha__ was set at 0.05, we would be willing to __reject__ the null hypothesis if, assuming the null hypothesis __is__ true, there's less than a 5% chance we would have observed our results in a random sample. 

So let's return to the Shapiro-Wilk normality test on our IQ sample
```{r}
shapiro.test(dt$IQ)

```

The Shapiro-Wilk normality test returned a p-value of __~.78__. That means there is a 78% chance we would see data that looks like our sample, assuming the null hypothesis __H~o~__ is true. __.78__ > __.05__, thus we __fail to reject__ the null hypothesis that our data came from a normal distribution. 

__NOTE:__ this is __not__ the same as saying "therefore our data came from a normal distribution." All hypothesis testing of this sort allows us to say is "based upon the strength of our evidence, we cannot reasonably rule out the possibility that our data came from a normal distribution."

__So...is our data normally distributed or not?__

No way of knowing! BUT -- at least we can move forward knowing that there isn't strong evidence suggesting that our data did __not__ come from a normal distribution. 

### Interpreting results
The Shapiro-Wilk example was chosen to illustrate these ideas because, unlike other statistical tests discussed later, large p-values are usually what you're hoping for -- you typically hope there isn't large evidence suggesting your data isn't normal. All statistical tests involve null hypotheses, and how you define the null hypothesis determines how you will interpret your results. If we were doing hypothesis testing on our sample of the average heights of individuals from Durham vs. Chapel Hill, and measured a difference of 7cm between the cities, we might set up our hypotheses as:

  + __H~o~__: There is no difference in average height between Durham and Chapel Hill
  + __H~A~__: There is a difference in average height between Durham and Chapel Hill
  
In that case, if we ran a t-test (covered below) and got a p-value of 0.04, this would mean: assuming there was __no difference__ in the average height, there would only be a 4% chance we would have observed a difference of 7cm by random sampling. Since this is below our p-threshold of 5%, we would be justified in __rejecting the null hypothesis__. And again, this does not mean that a true difference exists. Just that you failed to find compelling evidence that a difference __didn't__ exist. *Not Guilty* does not mean *Innocent*. 


# Basic Analyses
Here are some of the basic analyses that may be useful for your AVB eye-tracking analysis. For each, we'll describe an example use-case and how to format your data and run the analysis in R. 

First off, think about what your goal is. Are you trying to:

+ Examine __relationships__ between variables: 
    + 2 variables: [correlation](#correlation) 
    + predict DV based on 2 or more variables: [multiple regression](#multiple-regression)
+ Compare __means__ between:
    + one group vs. set value: [one-sample t-test](#one-sample-t-test)
    + two groups, same subjects: [paired-samples t-test](#paired-samples-t-test)
    + two groups, different subjects: [independent samples t-test](#independent-samples-t-test)
    + 3+ groups, one IV, same subjects: [repeated measures ANOVA](#repeated-measures-ANOVA)
    + 3+ groups, one IV, different subjecst: [one-way ANOVA](#one-way-ANOVA)

##Examine relationships

***

### Correlation
Basic correlations are a measure of the relationship between two variables. A correlation will tell you the strength and direction of this relationship. A positive correlation means that as one variable increases, the other tends to increase as well; a negative correlation means that as one variable increases, the other variable tends to decrease. 

For instance, in our sample dataset, say you were interested in knowing whether IQ scores predict fixation time on the noses of images of athletes. The two variables you'd select from the dataset are:
  
  + IQ
  + noseAthlete
  
To run the correlation in R:
```{r}

# Compute the correlation coefficient, and determine if the relationship is significant
cor.test(dt$IQ, y=dt$noseAthlete)
```

This provides you a lot of information. All the way at the bottom you have __cor__. This is the __correlation coeffcient__, __*r*__ and tells you the direction and the strength of the correlation. 

  + 0 < __*r*__ < 1: positive correlation
  + -1 < __*r*__ < 0: negative correlation
  + |__*r*__| : strength of correlation
  
  
So a __*r*__ of 0.66 represents a moderately strong *positive* correlation.

__Is this correlation significant?__ Can we say that IQ significantly predicts fixation time on Athletes' noses?
Our code also ran a hypothesis test behind the scenes. The hypotheses were:

  + __H~o~__: true correlation between variables is 0
  + __H~a~__: true correlation between variables is not 0
  
Find the p-value in the output table and interpret the result in the context of these hypotheses. 

__Plotting the correlation:__

It often helps to plot the correlation to get a visual sense of how two variables may be related to each other. Here's the R code to plot the two variables we correlated above:
```{r}
ggplot(dt, aes(x=IQ, y=noseAthlete)) +
  geom_point(size=7, alpha=.5)
```

We can see that indeed there does seem to be a positive relationship between the variables. Subject's with lower IQ scores also spent less time fixating on the nose's of athletes. 

__Using one variable to predict the other__

In the next section we'll talk about using __multiple regression__ to build a model to predict one variable based on a combination of 2+ seperate variables. This same idea can also be applied when you're working with just 2 variables. You can use __linear regression__ to build a model that predicts one variable based on the value of a different variable. In our case, given this correlation we may be interested in building a model that predicts fixation time on athlete's nose based on IQ score.

```{r}
# code for running a basic linear model in R
lmMod = lm(noseAthlete ~ IQ, data=dt)

summary(lmMod)
```

This gives us a model for predicting noseAthlete based on IQ that takes the form 

__*y = mx + b*__

where $y$ is noseAthlete, $x$ is IQ score, $m$ is the IQ coeffecient, and $b$ is the intercept. Furthermore, this output tells us that IQ is a __significant__ predictor of noseAthlete (find the associated p-value in the output table). Using this formula, we can predict the fixation time on athletes' nose for a *new* subject if we know his/her IQ score. For example, if the new subject's IQ score was __116__ we could predict the average amount of time they would spend fixating on the nose of athletes as:

__*y = (1.032)116 + 200.35*__

__*y = 320.08*__

We'd predict noseAthlete to be __~320ms__. Looking at the plot above, does this prediction seem reasonable?


Let's add this line to the plot, along with the confidence intervals
```{r}
ggplot(dt, aes(x=IQ, y=noseAthlete)) +
  geom_point(size=7, alpha=.5) +
  geom_smooth(method=lm, color="red")
```

### Multiple Regression
__Multiple regression__ is using 2 or more variables to predict a DV. For instance, if you wanted to predict IQ scores based on height and age, you'd use multiple regression. The formula follows the same pattern as the simple linear regression above, only with more variables. In this example, our formula would look like:

__*IQ = w~1~age + w~2~height + b*__

Here, __w~1~__ and __w~2~__ represent the coeffecients (or *weights*) assigned to each of the variables in our model. Running multiple regression will tell us whether this *model* significantly predicts IQ or not. 

Let's build a mulitple regression model consisting of nose fixation time on Move stars, Athletes, and Nobel Laureats, to see if we can predict IQ scores. So, our:
    
  + __DV__: IQ score
  + __IVs__ (aka __Predictors__) :
    + noseMovie
    + noseAthlete
    + noseNobel


```{r}
# code for running a basic multiple regression model in R
lmMod = lm(IQ ~ noseMovie + noseAthlete + noseNobel, data=dt)

summary(lmMod)
```

The __p-value__ all the way down at the bottom is less than __0.05__, which tells us that *overall* our model (including all 3 variables) is significant. 

However, we might want to know which variables are contributing the most to our model's prediction accuracy. You can find this information by looking at the coefficents table. We see that out of all 3 variables, only __noseAthlete__ is a significant predictor in our model. This shouldn't be surprising, as we demonstrated that noseAthlete and IQ score were significantly correlated in our discussion of [correlations](#correlation) above. 

## Compare Means 

***

### One-sample t-test
A __one-sample t-test__ can be used to test whether the mean of *a single variable* is significantly different from a set value. 

For instance, IQ tests are normalized so that the average score for a population should be 100. Let's use a one-sample t-test to test whether our sample of IQ scores is significantly different from the mean.  

```{r}
# code for one-sample t-test
t.test(dt$IQ, mu=100)

```
Whoa, interesting. Remember, we are testing against the null hypothesis, which in this case is __true mean of the population is 100__. Our __p-value__ is less than 0.05 (albeit slightly), so we are justified in __rejecting the null hypothesis__.  

And indeed, if we plot the distribution of our data, you can see that the sample mean (red) is a bit lower than the population mean (green)
```{r}
# plot the histogram for IQ scores
ggplot(dt, aes(x=IQ)) + 
  geom_histogram(binwidth=5, size=.2, col="white") + 
  geom_vline(xintercept=mean(dt$IQ), color="red") +
  geom_vline(xintercept=100, color="green") +
  ggtitle("Histogram for IQ (40 samples)")
```


### Paired-samples t-test

Instead of comparing the mean against a fixed number, say you wanted compare the means of __two__ variables to see if they are significantly different from each other. A t-test is perfect for this, but how you set it up depends critically on how each variable was collected. Are the same subjects represented in each variable? If so, use a __paired t-test__. Are the variables collected from different subjects? Use an __independent sample t-test__ (next section). 

The reason this is important is because your analysis needs to be different if the variables are __not__ independent from one another. Imagine we wanted to know if the amount of time spent fixating on the nose was different between images of athletes vs. Nobel laureates. We designed a study where we showed each participant images of athletes and images of Nobel laureates, and recorded the time spent fixating on the note. Say one of our participants really likes noses, and stares at the nose for the entire trial, regardless of whether it's an athlete or a Nobel laurete. This illustrates how a characteristic of a subject can exert an influence on __*both*__ measures. Because of this, the two variables are not *independent* of one another.

Here's the code to run this paired-samples t-test in r:
```{r}
# paired samples t-test
t.test(dt$noseAthlete, dt$noseNobel, paired=TRUE)
```

The __p-value__ indicates we have a significant difference between the conditions. 

Let's plot the means of each condition to visualize this result more clearly

```{r}
# we first need to convert these variables in the table from wide to long format (more on this under ANOVAs)
library(reshape2)
dt$ID <- 1:nrow(dt)    # create ID col
dt_long <- melt(dt, id.vars="ID", measure.vars = c("noseAthlete", "noseNobel"), variable.name="imageType")

# make box plot of the two measures
ggplot(dt_long, aes(x=imageType, y=value)) +
  geom_boxplot(fill="#A4A4A4") +
  ylim(0, 500)

```

__NOTE:__: For this example we chose 2 of the image categories in our dataset, but our dataset really has 3 levels of IV; it includes movie star images as well. If this were a real-life example, the more proper thing to do would be to compare the means across all 3 images categories (__ANOVAs__, described later). Deliberately leaving a group out of your analyses is only ever justified if you have strong *a priori* hypotheses about your results. 

### Independent Samples t-test
An __independent samples t-test__ allows you to compare the means of two *independent variables*, meaning they did not come from the same individuals. 

Since our example dataset has multiple measures collected from the same subjects, we can't run an independent samples t-test on it. Instead, let's make up some other fake data quick. Let's pretend we measured the heights of 200 people from Durham, and 200 people in Chapel Hill
```{r}
# set random number seed for later reproducibility
set.seed(500)

# generate sample data from normal distributions
durhamHeights = rnorm(200, mean=68, sd=5)
durm = rep("durm", 200)
chapHeights = rnorm(200, mean=66.8, sd=6)
ch = rep("ch", 200)

# make dataframe
city <- c(durm, ch)
heights <- c(durhamHeights, chapHeights)
height_dt <- data.frame(city, heights)

# plot the two distibutions
ggplot(height_dt, aes(x=heights, fill=city)) +
  geom_density(alpha=0.3) +
  xlim(40, 90) +
  ggtitle("Distribution of heights")

```

The two distributions looks pretty similar, with the mean height for Durham being maybe slighty taller. Let's run some stats on this to see if there is a significant difference. 

```{r}
# independent samples t-test
t.test(durhamHeights, chapHeights, paired=FALSE)
```

Our __p-value__ is above our threshold of 0.05, thus we __fail to reject the null hypothesis__, meaning we don't have sufficient evidence to rule out the possibility that the heights are the same in both cities. 


### Repeated measures ANOVA
Earlier we discussed how to compare the means of two variables that were measured from the same population (*see* [paired-samples t-test](#paired-samples-t-test)). What if you have three (or more) variables that you'd like to compare. For instance, in the paired-samples t-test discussion we compared fixation time on the noses of athletes vs Nobel lauretes. But we also have a third category: movie stars. If this was our experimental design, the appropriate thing to do would be to compare the dependent variable (nose fixation time) across all 3 levels of our independent variable (movie stars, athletes, Nobel laureates). A t-test won't cut it. We need to use a related test called __analysis of variance__, or __ANOVA__. 

Since our variables represent repeated observations made on the same subject, the particular flavor of ANOVA we need is __repeated measures ANOVA__. A repeated measures ANOVA is akin to a paired samples t-test for when you have three or more variables you'd like to compare. 

An __ANOVA__ will report whether the means of *any* of your variables are different from each other, but it won't tell you which ones. If (__and only if__) your ANOVA is significant, you can begin to do pair-wise comparisions to directly test which variables are different from each other. But if your ANOVA __isn't__ significant, these pair-wise tests aren't justified.

Say you want to test the hypothesis that fixation time on the nose of movie stars is greater than the fixation time on the nose of athletes. First, we have to run the __repeated measures ANOVA__ to see if there is a significant different between *any* of our image categories. 

First we have to reformat our data to *long* format. Instead of one subject per row, and multiple observations under different conditions along the columns, we want each row to represent a single observation, and a new variable that describes which condition it belongs to. Here's a portion of our *wide* format table:

```{r results=TRUE}
head(dt)

```

Here's how to reshape it to *long* format and view it again:
```{r}
# first reshape our datatable so one col is the DV (nose fixation time), and another describes the levels of the IV (image category)
dt_long <- melt(dt, id.vars="ID", measure.vars = c("noseMovie", "noseAthlete", "noseNobel"), variable.name="imageType", value.name="noseFixTime")
dt_long$ID = factor(dt_long$ID)    # make the ID column a factor variable

head(dt_long)
```

You can think of the ID column as subject name. Each unique ID is repeated 3 times in the full table, once for every unique imageType. Now that the data is formatted properly, we can run the repeated measures ANOVA:

```{r}
library(nlme)

rmANOVA <- lme(noseFixTime ~ imageType, random = ~1 | ID/imageType, data=dt_long, method="ML")
summary(rmANOVA)
```

The __p-value__ indicates that there is a significant effect of __imageType__ on __nose fixation time__. But we have 3 categories of imageType. How do these categories differ from each other. Since our ANOVA was significant, we are justified in comparing the categories against each other. We do so though what are called __post hoc t-tests__. There's a convenient R function to run post-hoc t-tests on your ANOVA results:

```{r warning=FALSE, message=FALSE}
# install.packages("multcomp")
library(multcomp)

postHoc <- glht(rmANOVA, linfct=mcp(imageType="Tukey"))
summary(postHoc)

```

Looking at this output you can see pair-wise comparisons between all 3 levels of our imageType variable. The __p-values__ in this table indicate that the nose fixation time was significantly different between all 3 image categories. Let's make a box plot to visualize this better:

```{r}

ggplot(dt_long, aes(x=imageType, y=noseFixTime)) +
  geom_boxplot(fill="#A4A4A4") + 
  ylim(0,500)

```


### One-way ANOVA
Just like a __repeated measures ANOVA__ is akin to a __paired-samples t-test__ for when you have 3 or more observations from the same subjects, a __one-way ANOVA__ is akin to a __independent samples t-test__ for when you have multiple observations from different subjects. 

Let's return to our earlier question of who's taller: Durhamites, or Chapel Hillionians. Say we wanted to expand the scope of our study and measure Raleighanders as well. Let's make-up that data and look at the distribututions of all 3 groups. 

```{r}
# generate sample data from normal distributions
raleighHeights = rnorm(200, mean=66, sd=5)
ral = rep("ral", 200)


# make dataframe
city <- c(durm, ch, ral)
heights <- c(durhamHeights, chapHeights, raleighHeights)
height_dt <- data.frame(city, heights)

# specify group colors to match the earlier durh/ch distribution
city.colors = c(ral="#53B400", ch="#F8766D", durm="#00B6EB")

# plot the two distibutions
ggplot(height_dt, aes(x=heights, fill=city)) +
  geom_density(alpha=0.3) +
  xlim(40, 90) +
  ggtitle("Distribution of heights by city") +
  scale_fill_manual(values=city.colors)

```

It looks like the mean height for Raleigh is even maybe shorter than Chapel Hill or Durham. Running a __one-way ANOVA__ will help us examine this question better. 

```{r}
owANOVA <- aov(heights ~ city, data=height_dt)
summary(owANOVA)
```

The __p-value__ indicates there is a significant effect of city on heights. We are justified in exploring this further through post hoc t-tests. For a one-way ANOVA between subjects, this is even easier:

```{r}
TukeyHSD(owANOVA)
```

This report shows us the pair-wise comparisons between the heights of each city. The only comparison that is significant (at our __p-value__ threshold of 0.05) is Raleigh - Durham. Let's plot the means by city. 

```{r}

ggplot(height_dt, aes(x=city, y=heights)) +
  geom_boxplot(fill="#A4A4A4") + 
  ylim(0,90)

```

So we can report that the average height of individuals from Raleigh is significantly less than the average height of individuals from Durham. There are no other significant differences found in this sample. 
