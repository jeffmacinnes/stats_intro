---
title: "Basic Stats"
author: "jeff macinnes"
date: "January 24, 2017"
header-includes:
   - \usepackage{bbm}
output: 
  html_document:
    toc: true
    mathjax: "default"
  github_document:
    toc: true
---

The goal here is to provide an overview on some basic statistical approaches that you can use in analyzing your AVB eye-tracking data. We'll cover the ideas behind these approaches -- identifying dependent/independent variables, deciding on appropriate analyses, interpretting results -- as well as the tools to run these analyses yourself. We'll be using [RStudio](https://www.rstudio.com/products/rstudio/download/), so if you haven't already download and install both [R](https://cran.r-project.org/) and [RStudio](https://www.rstudio.com/products/rstudio/download/).

# The Data
In order to have some data to play with, there is a simulated datafile in `data/exampleData.tsv`

This dataset has 40 subjects. For each subject we have values for IQ, and fixation time on the nose of various image categories. So we have:

* Dependent Variables (DV):
    + __IQ__: subject IQ score
    + __noseDwellTime__: time spent fixating on the nose
  
And we measured __noseDwellTime__ across 3 different image categories. So, one independent variable (IV) with 3 levels

* Independent Variable levels:
    + __movie__: faces of movie stars
    + __athlete__: faces of athletes
    + __nobel__: faces of Nobel laureates


So, let's load the dataset:
```{r}
# if you haven't already, set your working directory to 'stats_intro'
#setwd("<dirPath>/stats_intro")

# read the data, store in variable called 'dt'
dt <- read.csv('data/exampleData.tsv', sep='\t', row.names=1)

# print the first few rows
head(dt, n=5)
```

You can see all of our variables. Each subject has an IQ score and average noseDwellTimes for Movie stars, Athletes, and Nobel. 

Notice that our table has one row per subject, and multiple *observations* for each subject in different columns. When the table is organized like this, it is referred to as *wide-format* 

To access specific columns of your data, use the following format:
```{r}
# <tableName>$<columnName>
dt$IQ
```

# Descriptive Statistics
Descriptive statistics are ways to summarize *your data*. That is, they don't infer any conclusions from your data to the larger population from which your subjects are drawn from (you'll need **inferential statistics** for that...don't worry, it's coming). Descriptive statistics can be things like the *mean*, *mode*, and *median* of a variable. 

### Quick data summary
Let's get some basic descriptive stats on this dataset.

```{r}
# if you need to install the psych library:
# install.packages("psych", dependencies=TRUE)
library(psych)  # this is an external library that has some really useful tools within it

# the 'describe' function comes with the psych library and will report descriptive stats on all vars in your table
describe(dt)
```

You can see, for instance, that subjects spent an average of __249.81__ ms looking at the noses of movie stars, and __342.80__ ms looking at the noses of Nobel laureates. In this sample, participants spent longer (on average) looking at the noses of Nobel winners than movie stars. But that's all you can say. You cannot generalize that conclusion to the population at large. 

### Plotting raw data
It's often useful to plot your raw data to give you a better look at it. For one-dimensional data (e.g. subject IQ scores), a histogram can be a useful way to visualize your data
```{r warning=FALSE, message=FALSE} 
# if you need to install the ggplot library:
# install.packages("ggplot2", dependencies=TRUE)
library(ggplot2)  # useful plotting library

qplot(IQ, data=dt, 
      binwidth=5,
      main="Histogram for IQ (40 samples)",
      col=I("white"))
```

This shows the number of particpants (y-axis) whose IQ was within a certain IQ range (x-axis). Try plotting the distributions of other variables. By looking at the histogram, you can get a quick sense of whether a given variable is __normally distributed__. Many of the statistical tests described below rely and assume on data points being normally distributed. So what's that mean?

### Normal Distributions
If a variable is __normally distributed__, its histogram will take on a bell-shaped curve. Most of the data points will be clustered around the mean, i.e. the __count__ (i.e. y-axis) will have the greatest value near the mean. As you move outward toward the tails in either direction, the counts will drop off symmetrically. This means you have ever fewer datapoints at extreme values along the x-axis. 

Let's look at our IQ scores again. Above, we plotted the histogram of IQ scores in our sample of 40 subjects. Let's plot the histogram of IQ scores from a larger population, say 500 subjects.

```{r}
# read in larger population datatable
dt_pop <- read.csv('data/exampleData_population.tsv', sep='\t', row.names=1)


# plot the histogram for IQ scores from larger population
ggplot(dt_pop, aes(x=IQ, y=..density..)) + 
  geom_histogram(binwidth=5, size=.2, col="white") + 
  geom_density(colour="black") +
  geom_vline(xintercept=mean(dt_pop$IQ), color="red") +
  ggtitle("Histogram for IQ (500 samples)")

```

With 500 datapoints it becomes easier to see that IQ scores appear normally distributed. IQ scores are clustered symmetrically about the mean (shown as vertical red line). Offically, for a distribution to be considered __normal__:
  
  + __68%__ of datapoints must fall within +/- 1 standard deviation of the mean
  + __95.5%__ of datapoints must fall within +/- 2 standard deviations of the mean
  + the remaining __4.5%__ of datapoints are in the tails (__~2.3%__ in each tail)

Simply saying a distribution *looks* normal isn't enough. Ideally, we'd like to be able to *quantify* our evidence for making that claim. Testing whether a distribution is normal or not is a good segue into talking about __inferential statistics__. 

# Inferential Statistics
Inferential statistics are used to *infer* aspects about a population based on the *sample* of data you measured. You want to know if people are taller in Durham vs. Chapel Hill? You collect a random sample of 200 individuals from both locations, measure their heights, and come up with an average height of individuals from Durham and an average height of individuals from Chapel Hill. Say you measure that Durhamites are 3cm taller. Is this a __true__ difference, or just a result of the individuals you happened to sample? If you sampled 200 *different* people, how likely would you be to get the same response? This is where __inferential statistics__ come into play.

__So... are the IQ scores from our sample normally distributed or not?__

Here's the plot again

```{r echo=FALSE}
qplot(IQ, data=dt, 
      binwidth=5,
      main="Histogram for IQ (40 samples)",
      col=I("white"))
```

The Shapiro-Wilk normality test is a way of testing how liklely it is your observed data came from a normal distribution
```{r}
shapiro.test(dt$IQ)

```

This test, like many of the statistical tests covered below, returns a __p-value__. Knowing how to interpret a p-value is critical to understanding your results, and crucially, the limitations of your conclusions. To interpret a p-value, you need to be familiar with the idea of hypothesis testing.

### Hypothesis testing
With inferential statistics, the __true__ state of the world is unknown. You don't know the __true__ average height of everyone in Durham (unless you were to measure everyone, in which case you don't need stats). What you are left with is __probabilities__. How likely is it that my observations *reflect* the true state of the world? 

If we're thinking about our normal distribution question above, we have two possibilities. Or let's call them *hypotheses*:

  + __hypothesis 1:__ the data __really are__ normally distributed
  + __hypothesis 2:__ the data __really are *not* __ normally distributed
  
You can either conclude that hypothesis 1 is correct, or conclude that hypothesis 2 is correct. So, there are 4 possible outcomes:

|               | __truth:__ normal   | __truth:__ not normal  |
| ------------- |:-------------:| -----:|
| __conclude: normal__      | correct | incorrect (false positive) |
| __conclude: not normal__       | incorrect (false negative)      |   correct |

To set up a hypothesis test, we define one of our hypothesis as the __null__ hypothesis (called __H~o~__) and the other the __alternative__ hypothesis (__H~A~__). In the case of the Shapiro-Wilk normality test, we have:

  + __H~o~__: Our IQ scores came from a normally distributed population
  + __H~A~__: Our IQ scores did not come from a normally distributed population
  
So our null hypothesis is that our data indeed came from a normally distributed population. Remember, we don't have any way of knowing if this is __actually__ true or not. Instead, we say "assuming the data __DID__ come from a normally distributed population, how probable is it that we would have gotten our observed data through random sampling?" This probability is reflected in the __p-value__. 

When we run a hypothesis test we are asking whether we are going to __reject__ or __fail to reject__ the null hypothesis based on the strength of the p-value. So we also need to set up a p-value threshold that will allow us to make that decision. This is often set at p < 0.05; or, in other words, we are willing to __reject__ the null hypothesis if, assuming the null hypothesis __is__ true, there's less than a 5% chance we would have observed our results in a random sample. 

So let's return to the Shapiro-Wilk normality test on our IQ sample
```{r}
shapiro.test(dt$IQ)

```

The Shapiro-Wilk normality test returned a p-value of __~.78__. That means there is a 78% chance we would see data that looks like our sample, assuming the null hypothesis __H~o~__ is true. __.78__ > __.05__, thus we __fail to reject__ the null hypothesis that our data came from a normal distribution. 

__NOTE:__ this is __not__ the same as saying "therefore our data came from a normal distribution." All hypothesis testing of this sort allows us to say is "based upon the strength of our evidence, we cannot reasonably rule out the possibility that our data came from a normal distribution."

__So...is our data normally distributed or not?__

No way of knowing! BUT -- at least we can move forward knowing that there isn't strong evidence suggesting that our data did __not__ come from a normal distribution. 

### Interpreting results
The Shapiro-Wilk example was chosen to illustrate these ideas because, unlike other statistical tests discussed later, large p-values are usually what you're hoping for -- you typically hope there isn't large evidence suggesting your data isn't normal. All statistical tests involve null hypotheses, and how you define the null hypothesis determines how you will interpret your results. If we were doing hypothesis testing on our sample of the average heights of individuals from Durham vs. Chapel Hill, and measured a difference of 7cm between the cities, we might set up our hypotheses as:

  + __H~o~__: There is no difference in average height between Durham and Chapel Hill
  + __H~A~__: There is a difference in average height between Durham and Chapel Hill
  
In that case, if we ran a t-test (covered below) and got a p-value of 0.04, this would mean: assuming there was __no difference__ in the average height, there would be a 4% chance we would have observed a difference of 7cm. Since this is below our p-threshold of 5%, we would be justified in __rejecting the null hypothesis__. And again, this does not mean that a true difference exists. Just that you failed to find compelling evidence that a difference __didn't__ exist. *Not Guilty* does not mean *Innocent*. 


# Basic Analyses
Here are some of the basic analyses that may be useful for your AVB eye-tracking analysis. For each, we'll describe an example use-case and how to format your data and run the analysis in R. 

First off, think about what your goal is. Are you trying to:

+ Examine __relationships__ between variables: 
    + 2 variables: [correlation](#correlation) 
    + predict DV based on 2 or more variables: __multiple regression__
+ Compare __means__ between:
    + one group vs. set value: __one-sample t-test__
    + two groups, same subjects: __paired-samples t-test__
    + two groups, different subjects: __independent samples t-test__
    + >2 groups, one IV, same subjects: __repeated measures ANOVA__
    + >2 groups, one IV, different subjecst: __one-way ANOVA__

## Correlation
Basic correlations are a measure of the relationship between two variables. A correlation will tell you the strength and direction of this relationship. A positive correlation means that as one variable increases, the other tends to increase as well; a negative correlation means that as one variable increases, the other variable tends to decrease. 

For instance, in our sample dataset, say you were interested in knowing whether IQ scores predict fixation time on the noses of images of athletes. The two variables you'd select from the dataset are:
  
  + IQ
  + noseAthlete
  
To run the correlation in R:
```{r}

# Compute the correlation coefficient, and determine if the relationship is significant
cor.test(dt$IQ, y=dt$noseAthlete)
```

This provides you a lot of information. All the way at the bottom you have __cor__. This is the __correlation coeffcient__, __*r*__ and tells you the direction and the strength of the correlation. 

  + 0 < __*r*__ < 1: positive correlation
  + -1 < __*r*__ < 0: negative correlation
  + |__*r*__| : strength of correlation
  
  
So a __*r*__ of 0.66 represents a moderately strong *positive* correlation.

__Is this correlation significant?__ Can we say that IQ significantly predicts fixation time on Athletes' noses?
Our code also ran a hypothesis test behind the scenes. The hypotheses were:

  + __H~o~__: true correlation between variables is 0
  + __H~a~__: true correlation between variables is not 0
  
Find the p-value in the output table and interpret the result in the context of these hypotheses. 

__Plotting the correlation:__

It often helps to plot the correlation to get a visual sense of how two variables may be related to each other. Here's the R code to plot the two variables we correlated above:
```{r}
ggplot(dt, aes(x=IQ, y=noseAthlete)) +
  geom_point(size=7, alpha=.5)
```

We can see that indeed there does seem to be a positive relationship between the variables. Subject's with lower IQ scores also spent less time fixating on the nose's of athletes. 

__Using one variable to predict the other__

In the next section we'll talk about using __multiple regression__ to build a model to predict one variable based on a combination of 2+ seperate variables. This same idea can also be applied when you're working with just 2 variables. You can use __linear regression__ to build a model that predicts one variable based on the value of a different variable. In our case, given this correlation we may be interested in building a model that predicts fixation time on athlete's nose based on IQ score.

```{r}
# code for running a basic linear model in R
lmMod = lm(noseAthlete ~ IQ, data=dt)

summary(lmMod)
```

This gives us a model for predicting noseAthlete based on IQ that takes the form 

$${y=mx+b}$$

where $y$ is noseAthlete, $x$ is IQ score, $m$ is the IQ coeffecient, and $b$ is the intercept. Furthermore, this output tells us that IQ is a __significant__ predictor of noseAthlete (find the associated p-value in the output table). Using this formula, we can predict the fixation time on athletes' nose for a *new* subject if we know his/her IQ score. For example, if the new subject's IQ score was __116__ we could predict the average amount of time they would spend fixating on the nose of athletes as:

$$
\begin{align}
y &= 1.032 \cdot 116 + 200.3546\\
y &= {320.08}
\end{align}
$$


$$\sum_{i=1}^{n} X^3_i$$

This is just \textit{n} practice
We'd predict noseAthlete to be ~320ms. Looking at the plot above, does this prediction seem reasonable?


Let's add this line to the plot, along with the confidence intervals
```{r}
ggplot(dt, aes(x=IQ, y=noseAthlete)) +
  geom_point(size=7, alpha=.5) +
  geom_smooth(method=lm, color="red")
```

## Multiple Regression